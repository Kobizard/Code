import os
import re
import pandas as pd
from IPython.display import display

# Step 1: Auto-detect the M code file (Excel or CSV) with required columns.
folder_path = os.getcwd()
sheet_df = None

for f in os.listdir(folder_path):
    if f.endswith(".xlsx") and not f.startswith("~$"):
        try:
            df = pd.read_excel(f)
            if "Query_name" in df.columns and "M_expression" in df.columns:
                sheet_df = df
                print(f"‚úÖ Loaded file: {f}")
                break
        except Exception as e:
            print(f"Error reading {f}: {e}")
    elif f.endswith(".csv"):
        try:
            df = pd.read_csv(f)
            if "Query_name" in df.columns and "M_expression" in df.columns:
                sheet_df = df
                print(f"‚úÖ Loaded file: {f}")
                break
        except Exception as e:
            print(f"Error reading {f}: {e}")

if sheet_df is None:
    raise FileNotFoundError("‚ùå Could not find a file with 'Query_name' and 'M_expression' columns.")

# Build a set of all query names from the file.
all_query_names = set(sheet_df["Query_name"].astype(str).str.strip())

# Step 2: Define patterns to detect sources
patterns = [
    (r'\[([^\]]+)\]\.\[([^\]]+)\]', lambda m: f"[{m.group(1)}].[{m.group(2)}]"),
    (r'SharePoint\.Files\("([^"]+)"', lambda m: f"SharePoint ({m.group(1)})"),
    (r'Excel\.Workbook\(([^,\)]+)', lambda m: f"Excel Workbook ({m.group(1).strip()})"),
    (r'Databricks\.Catalogs\([^)]+\)', lambda m: "Databricks (Source)"),
    (r'Table\.FromRows|List\.Max|List\.Generate|#"\w+"', lambda m: "In-Memory / Generated Table"),
    (r'FROM\s+([a-zA-Z0-9_]+\.[a-zA-Z0-9_]+)', lambda m: f"{m.group(1)} (SQL Table)"),
    # Databricks Table
    (r'Name\s*=\s*"(?P<table>[a-zA-Z0-9_]+)",\s*Kind\s*=\s*"Table"\]\}\[Data\]', lambda m: f"{m.group('table')} (Databricks Table)"),
    # Databricks Schema
    (r'(\w+)\s*=\s*\w+\{\[Name\s*=\s*"(?P<schema>[a-zA-Z0-9_]+)",\s*Kind\s*=\s*"Schema"\]\}\[Data\]', lambda m: f"{m.group('schema')} (Databricks Schema Ref)")
]

# Step 3: Extract direct sources from each query
query_sources = {}

for _, row in sheet_df.iterrows():
    query_name = str(row["Query_name"]).strip()
    expression = str(row["M_expression"]).strip()
    found_sources = set()

    # Detect direct full query references
    if expression.startswith("=") and "(" not in expression:
        internal_ref = expression[1:].strip()
        if internal_ref in all_query_names:
            found_sources.add(internal_ref)

    # Apply all source detection patterns
    for pattern, formatter in patterns:
        for match in re.finditer(pattern, expression):
            found_sources.add(formatter(match))

    # Detect in-expression query references
    for other_query in all_query_names:
        if other_query == query_name:
            continue
        if re.search(rf'\b#?"?{re.escape(other_query)}"?\b', expression):
            found_sources.add(other_query)

    if not found_sources:
        found_sources.add("No source detected")

    query_sources[query_name] = list(found_sources)

# Step 4: Map schema+table if Databricks found
def databricks_schema_table_merge(sources):
    schema = None
    tables = []
    other_sources = []

    for s in sources:
        if s.endswith("(Databricks Schema Ref)"):
            schema = s.split(" ")[0]
        elif s.endswith("(Databricks Table)"):
            tables.append(s.split(" ")[0])
        else:
            other_sources.append(s)

    # Combine schema + table
    if schema and tables:
        combined = [f"Databricks-{schema}.{t}" for t in tables]
        return other_sources + combined
    else:
        return other_sources + tables

# Step 5: Recursively resolve true sources and depth
def resolve_sources(query, visited=None):
    if visited is None:
        visited = set()
    if query in visited:
        return set()
    
    current_sources = query_sources.get(query, [])
    resolved = set()

    for src in current_sources:
        if src in all_query_names:
            resolved.update(resolve_sources(src, visited | {query}))
        else:
            resolved.add(src)

    return set(databricks_schema_table_merge(resolved))

# Step 6: Calculate depth of each query (longest path to raw data)
def calculate_depth(query, visited=None):
    if visited is None:
        visited = set()
    if query in visited:
        return 0
    visited = visited | {query}
    depth = 0
    for src in query_sources.get(query, []):
        if src in all_query_names:
            depth = max(depth, 1 + calculate_depth(src, visited))
    return depth

# Step 7: Final flatten
results = []
for query in sorted(all_query_names):
    final_sources = resolve_sources(query)
    depth = calculate_depth(query)
    for src in sorted(final_sources):
        results.append((query, src, depth))

df_result = pd.DataFrame(results, columns=["Query Name", "Table/Folder", "Query Depth"])

# Step 8: Display and export
display(df_result)
df_result.to_excel("Resolved_Sources_by_Query_with_Depth.xlsx", index=False)
print("üìÅ Results saved to 'Resolved_Sources_by_Query_with_Depth.xlsx'")
