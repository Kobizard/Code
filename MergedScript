import os
import re
import pandas as pd
from collections import defaultdict
import stat

# === Step 1: Locate the Excel file ending with "_Lineage.xlsx" ===
folder_path = os.getcwd()
lineage_file = None

for f in os.listdir(folder_path):
    if f.endswith("_Lineage.xlsx") and not f.startswith("~$"):
        lineage_file = f
        break

if lineage_file is None:
    raise FileNotFoundError("‚ùå No Excel file ending with '_Lineage.xlsx' found in the folder.")

print(f"üìÑ Found Excel file: {lineage_file}")

# === Step 2: Read M expressions from appropriate sheet ===
xls = pd.ExcelFile(lineage_file)
df_m_expressions = None
for sheet_name in xls.sheet_names:
    df = pd.read_excel(xls, sheet_name=sheet_name)
    if "Query_name" in df.columns and "M_expression" in df.columns:
        df_m_expressions = df
        break

if df_m_expressions is None:
    raise ValueError("‚ùå Could not find a sheet with 'Query_name' and 'M_expression'.")

all_query_names = set(df_m_expressions["Query_name"].astype(str).str.strip())

# === Source extraction patterns ===
patterns = [
    (r'\[([^\]]+)\]\.\[([^\]]+)\]', lambda m: f"[{m.group(1)}].[{m.group(2)}]"),  # M-style bracketed
    (r'SharePoint\.Files\("([^"]+)"', lambda m: f"SharePoint ({m.group(1)})"),
    (r'Excel\.Workbook\(([^,\)]+)', lambda m: f"Excel Workbook ({m.group(1).strip()})"),
    (r'Databricks\.Catalogs\([^)]+\)', lambda m: "Databricks (Source)"),
    (r'Name\s*=\s*"(?P<table>\w+)",\s*Kind\s*=\s*"Table"\]\}\[Data\]', lambda m: f"{m.group('table')} (Databricks Table)"),
    (r'(\w+)\s*=\s*\w+\{\[Name\s*=\s*"(?P<schema>\w+)",\s*Kind\s*=\s*"Schema"\]\}\[Data\]', lambda m: f"{m.group('schema')} (Databricks Schema Ref)"),
    (r'Table\.FromRows|List\.Max|List\.Generate|#"\w+"', lambda m: "In-Memory / Generated Table"),
    (r'\b(?:FROM|JOIN)\s+([A-Za-z0-9_]+)\.\"{2}([^"]+)\"{2}', lambda m: f"{m.group(1)}.{m.group(2)}"),  # SAP HANA style
    (r'\b(?:FROM|JOIN)\s+([A-Za-z0-9_]+)\.([A-Za-z0-9_\-]+)', lambda m: f"{m.group(1)}.{m.group(2)}"),  # schema.table
    (r'\b(?:FROM|JOIN)\s+\[([^\]]+)\]\.\[([^\]]+)\]', lambda m: f"{m.group(1)}.{m.group(2)}"),  # T-SQL bracketed
]

# === Source detection ===
query_sources = {}
for _, row in df_m_expressions.iterrows():
    query_name = str(row["Query_name"]).strip()
    expression = str(row["M_expression"]).strip()
    expression = expression.replace("#(lf)", "\n").replace("#(tab)", "\t").replace("#(cr)", "\r")
    found_sources = set()

    if expression.startswith("=") and "(" not in expression:
        ref = expression[1:].strip()
        if ref in all_query_names:
            found_sources.add(ref)

    for pattern, format_fn in patterns:
        for match in re.finditer(pattern, expression, flags=re.IGNORECASE | re.MULTILINE):
            found_sources.add(format_fn(match))

    for other in all_query_names:
        if other != query_name and re.search(rf'\b#?"?{re.escape(other)}"?\b', expression):
            found_sources.add(other)

    query_sources[query_name] = list(found_sources)

def merge_databricks_sources(sources):
    schema = None
    tables = []
    others = []
    for s in sources:
        if s.endswith("(Databricks Schema Ref)"):
            schema = s.split(" ")[0]
        elif s.endswith("(Databricks Table)"):
            tables.append(s.split(" ")[0])
        else:
            others.append(s)
    if schema and tables:
        return others + [f"Databricks-{schema}.{t}" for t in tables]
    return others + tables

def resolve_sources(query, visited=None):
    if visited is None:
        visited = set()
    if query in visited:
        return set()
    direct = query_sources.get(query, [])
    resolved = set()
    for src in direct:
        if src in all_query_names:
            resolved.update(resolve_sources(src, visited | {query}))
        else:
            resolved.add(src)
    resolved = {r for r in resolved if r != "No source detected"}
    return set(merge_databricks_sources(resolved)) or {"No source detected"}

records = []
for query in sorted(all_query_names):
    for source in sorted(resolve_sources(query)):
        records.append((query, source))

df_sources = pd.DataFrame(records, columns=["Query Name", "Table/Folder"])

# === Step 3: BiDirectional Dependency Graph ===
dependencies = defaultdict(set)
for _, row in df_m_expressions.iterrows():
    query = str(row["Query_name"]).strip()
    expression = str(row["M_expression"]).strip()
    if expression.startswith("=") and "(" not in expression:
        ref = expression[1:].strip()
        if ref in all_query_names:
            dependencies[query].add(ref)
    for other_query in all_query_names:
        if other_query != query and re.search(rf'\b#?"?{re.escape(other_query)}"?\b', expression):
            dependencies[query].add(other_query)

used_by = defaultdict(set)
for query, deps in dependencies.items():
    for dep in deps:
        used_by[dep].add(query)

def get_all_upstream(query, dependency_map, visited=None):
    if visited is None:
        visited = set()
    result = set()
    for dep in dependency_map.get(query, []):
        if dep not in visited:
            visited.add(dep)
            result.add(dep)
            result.update(get_all_upstream(dep, dependency_map, visited))
    return result

def get_all_downstream(query, reverse_map, visited=None):
    if visited is None:
        visited = set()
    result = set()
    for user in reverse_map.get(query, []):
        if user not in visited:
            visited.add(user)
            result.add(user)
            result.update(get_all_downstream(user, reverse_map, visited))
    return result

relations = []
for query in sorted(all_query_names):
    for parent in get_all_upstream(query, dependencies):
        relations.append({"Query": query, "RelatedQuery": parent, "RelationType": "Parent"})
    for child in get_all_downstream(query, used_by):
        relations.append({"Query": query, "RelatedQuery": child, "RelationType": "Daughter"})
    if not dependencies[query] and not used_by[query]:
        relations.append({"Query": query, "RelatedQuery": query, "RelationType": "Parent"})

df_bidirectional = pd.DataFrame(relations)

# === Step 4: Visual Lineage ===
df_visuals = pd.read_excel(lineage_file, sheet_name="VisualResult")
df_sources = df_sources.rename(columns={'Query Name': 'Query', 'Table/Folder': 'Source Table'})
df_sources["Query"] = df_sources["Query"].astype(str).str.strip()
df_bidirectional["Query"] = df_bidirectional["Query"].astype(str).str.strip()
df_bidirectional["RelatedQuery"] = df_bidirectional["RelatedQuery"].astype(str).str.strip()

parent_map = df_bidirectional[df_bidirectional["RelationType"] == "Parent"] \
    .groupby("Query")["RelatedQuery"].apply(set).to_dict()

lineage_output = []
for _, visual_row in df_visuals.iterrows():
    visual_query = visual_row["ArtifactSource"].strip()
    visited = set()
    stack = [visual_query]
    while stack:
        current = stack.pop()
        if current in visited:
            continue
        visited.add(current)
        lineage_output.append({
            "Query": current,
            "VisualBaseQuery": visual_query,
            "Report Name": visual_row['Report Name'],
            "Page Name": visual_row['Page Name'],
            "Visual Name": visual_row['Visual Name'],
            "Visual Type": visual_row['Visual Type'],
            "ArtifactName": visual_row['ArtifactName'],
            "ArtifactType": visual_row['ArtifactType']
        })
        stack.extend(parent_map.get(current, []))

df_lineage = pd.DataFrame(lineage_output).drop_duplicates()

df_lineage = df_lineage.merge(
    df_bidirectional[df_bidirectional['RelationType'] == 'Parent'],
    how='left',
    on='Query'
).rename(columns={'RelatedQuery': 'Source'})

df_lineage = df_lineage.merge(
    df_sources,
    on='Query',
    how='left'
)

# === Step 5: Export to a new file ===
output_file = lineage_file.replace(".xlsx", "_Updated.xlsx")

existing_data = {}
with pd.ExcelFile(lineage_file, engine='openpyxl') as reader:
    for sheet in reader.sheet_names:
        if sheet not in ["DataSourceResult", "BiDirectionalQueries", "All Queries-Visual Lineage"]:
            existing_data[sheet] = pd.read_excel(reader, sheet_name=sheet)

with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:
    for sheet, df in existing_data.items():
        df.to_excel(writer, sheet_name=sheet, index=False)
    df_sources.to_excel(writer, sheet_name="DataSourceResult", index=False)
    df_bidirectional.to_excel(writer, sheet_name="BiDirectionalQueries", index=False)
    df_lineage.to_excel(writer, sheet_name="All Queries-Visual Lineage", index=False)

os.chmod(output_file, stat.S_IWRITE | stat.S_IREAD)

print(f"‚úÖ All sheets written safely to: {output_file}")
